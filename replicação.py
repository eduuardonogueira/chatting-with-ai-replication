# -*- coding: utf-8 -*-
"""replicação.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CfUc5Qe1SAIdRu-3DYMPwmFziga8fwqL

Baixar os datasets para a máquina do Colab
"""

# Instalar Git LFS e langdetect
!apt-get install -y git-lfs
!git lfs install
!pip install langdetect

# Criar pasta para os datasets
import os
dataset_folder = '/content/datasets/'
os.makedirs(dataset_folder, exist_ok=True)

# Lista de arquivos que queremos
files_to_download = [
    "20230727_195816_hn_sharings.json",
    "20230727_195927_pr_sharings.json",
    "20230727_195941_issue_sharings.json",
    "20230727_195954_discussion_sharings.json",
    "20230727_200003_commit_sharings.json",
    "20230727_200102_file_sharings.json"
]

# Criar um repositório temporário só para pegar os arquivos do LFS
!rm -rf temp_repo
!git init temp_repo
os.chdir('temp_repo')
!git remote add origin https://github.com/NAIST-SE/DevGPT.git
!git config core.sparseCheckout true

# Especificar os arquivos que queremos no sparse-checkout
with open(".git/info/sparse-checkout", "w") as f:
    for file in files_to_download:
        f.write(f"snapshot_20230727/{file}\n")

# Puxar apenas os arquivos especificados
!git pull origin main

# Mover os arquivos para datasets
import shutil
for file in files_to_download:
    src = os.path.join("snapshot_20230727", file)
    dst = os.path.join(dataset_folder, file)
    shutil.move(src, dst)

os.chdir('/content')
print(f"✅ Arquivos baixados em {dataset_folder}")

# # Opcional: baixar para sua máquina
# from google.colab import files  # para baixar direto para sua máquina
# for filename in urls.keys():
#     print(f"Preparando download de {filename} para sua máquina...")
#     files.download(save_folder + filename)

"""Replicação - Equipe

- snapshot_20230727
- pegaram 100 conversas para montar as categorias
- resultou em 22 categorias
- 95% de confiança e 5% de erro
- Qualquer conversa que não estiver disponível e não for em inglês, descarta (N/A)

Importando o dataset em json
"""

import os
import json
import pandas as pd

# Pasta onde estão os arquivos JSON
dataset_folder = './datasets/'

# Lista todos os arquivos JSON na pasta
json_files = [f for f in os.listdir(dataset_folder) if f.endswith('.json')]

all_sources = []

for file in json_files:
    file_path = os.path.join(dataset_folder, file)
    print(file_path)

    with open(file_path, 'r', encoding='utf-8') as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError:
            print(f"❌ Erro ao ler {file}")
            continue

    # Pega os dados da chave "Sources", se existir
    sources = data.get("Sources", [])

    # Adiciona uma coluna extra para identificar o arquivo de origem
    for source in sources:
        source["dataset_origin"] = file

    all_sources.extend(sources)

# Cria o DataFrame final
df = pd.DataFrame(all_sources)
print("Arquivos importados!")

print(f"✅ Total de registros: {len(df)}")
df.head()

"""percorre o JSON das fontes e conversas do ChatGPT, filtra apenas os primeiros prompts feitos pelos usuários e transforma em uma linha estruturada com metadados para análise em um DataFrame.

"""

rows = []

# Converte DataFrame em lista de dicionários
records = df.to_dict(orient="records")

# iterar e pegar somente o primeiro prompt de cada conversa
for src in records:
    chat_sharings = src.get("ChatgptSharing", []) or []
    for share in chat_sharings:
        conversations = share.get("Conversations", []) or []

        # Pega apenas o primeiro item, se existir
        if not conversations:
            continue

        first_turn = conversations[0]
        prompt = first_turn.get("Prompt")  # ajuste a chave se necessário

        if prompt is None:
            continue

        rows.append({
            "source_type": src.get("Type"),
            "source_url": src.get("URL"),
            "source_title": src.get("Title"),
            "chat_url": share.get("URL"),
            "chat_title": share.get("Title"),
            "date_of_conversation": share.get("DateOfConversation"),
            "status": share.get("Status"),
            "prompt": prompt
        })

df = pd.DataFrame(rows)

print(df.columns)
print(f"Total de prompts extraídos: {len(df)}")

"""Tratamentos:
*   Limpeza de texto
*   Remover prompts que não estão em inglês
*   Remover duplicatas


"""

import re
from langdetect import detect, LangDetectException

def detect_language(text):
    try:
        return detect(text)
    except LangDetectException:
        return "unknown"

if not df.empty:
    df["prompt"] = (
        df["prompt"]
        .astype(str)
        .str.replace(r"\s+", " ", regex=True)
        .str.strip()
    )
    df = df[df["prompt"].str.len() > 0].reset_index(drop=True)

print(f"Limpeza no texto: {len(df)}")
print(f"\nAntes dos filtros: {len(df)}")

# Classificando as conversas por idioma.
df["lang"] = df["prompt"].apply(detect_language)

# Manter somente conversas em Inglês no Dataset
df_lang = df[df["lang"] == "en"]

print(f"\nApós filtro de idioma (inglês): {len(df_lang)}")

# Realizar uma última verificação para garantir
# que os dados estejam filtrados de maneira adequada.
# Removendo NAN e prompt com menos de 5 caracteres
df = df_lang[
    df_lang["prompt"].notna() & (df_lang["prompt"].str.len() > 5)
].reset_index(drop=True)

# Remover duplicatas antes da amostragem
df = df.drop_duplicates(subset=["prompt"]).reset_index(drop=True)

print(f"\nApós remoção de duplicatas: {len(df)}")

print(f"\nDataset final após todos os filtros: {len(df)}")

"""Selecionar aleatoriamente 100 conversas para a fase exploratória."""

# Selecionar aleatoriamente 100 conversas para fase exploratória
trial_sample = df.sample(n=100, random_state=42)

# Gerar .csv
# trial_sample.to_csv("trial_phase.csv", index=False)

"""Amostra estatística de 321 conversas"""

# Retirar do dataset principal os 100 selecionados
remaining = df.drop(trial_sample.index)
# Do dataset restante, selecionar 321 para analise manual
coding_sample = remaining.sample(n=321, random_state=42)

# Gerar .csv
# coding_sample.to_csv("coding_phase.csv", index=False)

"""## Fase manual
Aqui será gerado o csv com 321 prompts para a classificação manual de categorias por dois avaliadores.
"""

# Criar as colunas para o preenchimento das categorias pelos 2 anotadores
coding_sample["annotator_A"] = ""
coding_sample["annotator_B"] = ""

# Gerar o .csv
coding_sample[[
    "chat_url",
    "prompt",
    "annotator_A",
    "annotator_B"
]].to_csv("annotation_sheet.csv", index=False)

"""## Cohen's Kappa

Aqui será feito o upload do csv preenchido pelas categorias definidas pelos anotadores e será calculada a concordância utilizando o Cohen's Kappa

é uma métrica estatística robusta que mede a concordância entre dois avaliadores (ou classificadores) que classificam itens em categorias nominais, corrigindo a concordância que ocorre por acaso
"""

from sklearn.metrics import cohen_kappa_score

annotated = pd.read_csv("annotation_sheet_filled.csv")

kappa = cohen_kappa_score(
    annotated["annotator_A"],
    annotated["annotator_B"]
)

print("Cohen's Kappa:", kappa)

"""## Consolidação dos rótulos"""

def resolve_label(row):
    if row["annotator_A"] == row["annotator_B"]:
        return row["annotator_A"]
    return row["annotator_B"]

annotated["label"] = annotated.apply(resolve_label, axis=1)
final_dataset = annotated[["prompt", "label"]]

"""## Pré-processamento textual"""

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re

nltk.download("stopwords")

stemmer = PorterStemmer()
stop_words = set(stopwords.words("english"))

def preprocess(text):
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)
    tokens = text.split()
    tokens = [stemmer.stem(t) for t in tokens if t not in stop_words]
    return " ".join(tokens)

final_dataset["clean_text"] = final_dataset["prompt"].apply(preprocess)

"""## Classificação automática"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(final_dataset["clean_text"])
y = final_dataset["label"]

"""## Treinamento dos modelos"""

from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

models = {
    "SVM": LinearSVC(),
    "Naive Bayes": MultinomialNB(),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42)
}

from sklearn.metrics import classification_report, confusion_matrix

for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)

    print(f"\nModelo: {name}")
    print(classification_report(y_test, preds, zero_division=0))

import matplotlib.pyplot as plt
import seaborn as sns

rf = models["Random Forest"]
cm = confusion_matrix(y_test, rf.predict(X_test))

plt.figure(figsize=(12, 8))
sns.heatmap(cm, cmap="Blues")
plt.title("Confusion Matrix – Random Forest")
plt.show()